{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "------------------------------\n",
    "### MARKKLICK206\n",
    "Mark Klick\n",
    "4/17/2018\n",
    "------------------------------\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#import urllib2\n",
    "import sys\n",
    "import time\n",
    "#import cPickle\n",
    "import urllib\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.metrics import precision\n",
    "from nltk.metrics import recall\n",
    "from nltk.metrics import f_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow this tutorial to set up twitter credentials for the Twitter API\n",
    "http://www.nltk.org/howto/twitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.twitter import Query, Streamer, Twitter, TweetViewer, TweetWriter, credsfromfile\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oauth = credsfromfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where I'll begin the twitter 'sentiment' classification i.e. use twitter handles - starting with 1 handle vs another handle - we can group several handles together and call that a group then compare the two groups as well...\n",
    "\n",
    "### Hopefully there will be some interesting/informative results when we try and classify sentences in our test set and correctly/incorrectly identify which twitter-handle/person those sentences came from...\n",
    "\n",
    "### Let's start with a 70-30 split and see how well the nltk.NaiveBayesClassifier will perform when trying to distinguish/classify tweets from two different twitter handles...\n",
    "\n",
    "### First we need to use our nltk twitter streamer to grab our twitter corpi for our two handles we are interested in (trump and obama in this first toy example)... and conveniently write those to 2 seperate json files... to make life easy when we convert them into tuples i.e. (sentences, twitter_handle)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://tweeterid.com/\n",
    "#trump-25073877, obama-813286\n",
    "\n",
    "# create our streamer client\n",
    "client = Streamer(**oauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create our trump-corpus with 500 tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_id =['25073877']\n",
    "\n",
    "client.register(TweetWriter(limit=500))\n",
    "client.statuses.filter(follow=trump_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at the raw tweet strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_strings = twitter_samples.strings('trump_tweets.json')\n",
    "#for string in t_strings[:50]:\n",
    "    #print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at the tokenzied tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_tokenized = twitter_samples.tokenized('trump_tweets.json')\n",
    "#for toks in t_tokenized[:50]:\n",
    "    #print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print len(t_strings)\n",
    "print len(t_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create our obama-corpus with 500 tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obama_id =['813286']\n",
    "\n",
    "client.register(TweetWriter(limit=500))\n",
    "client.statuses.filter(follow=obama_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o_strings = twitter_samples.strings('obama_tweets.json')\n",
    "#for string in o_strings[:50]:\n",
    "    #print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o_tokenized = twitter_samples.tokenized('obama_tweets.json')\n",
    "#for toks in o_tokenized[:50]:\n",
    "    #print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(o_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk twitter streamer is getting stuck when trying to grab 500 obama tweets - we'll just stick to 230 for now - the number of tweets is arbitrary once we get the pipeline up and running i.e. you can plug in any twitter handle and the amount of tweets you wish to grab for the corpus..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### start pre-processing our data into the format we like i.e. tuples (sentence,twitter-handle). 2 ways we can do this, I'll try both:\n",
    "\n",
    ">use the nltk.twitter_samples\n",
    "\n",
    ">use my pre-processing workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the string_class_tuple i.e. a tuple with (tweet, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_list = [(''.join(t_tweets), 'trump') for t_tweets in t_strings]\n",
    "\n",
    "obama_list = [(''.join(t_tweets), 'obama') for o_tweets in o_strings]\n",
    "\n",
    "pres_corpus = trump_list[:230] + obama_list[:230]\n",
    "\n",
    "random.shuffle(pres_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define our prepocessing pipepline and make it into a function\n",
    "> first we tokenize each tweet and then we stem each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_proc(string_class_tuple):\n",
    "    #preprocess the tweets with our existing token/stem pipeline?\n",
    "    #hold each processed sentence strings \n",
    "    train_set_proc = []\n",
    "    #bag of stemmed words \n",
    "    all_words = []\n",
    "    #iterate over each tweet string - tokenize sentences - tok. words - stem\n",
    "    for i in string_class_tuple:\n",
    "        doc = []\n",
    "        for sentence in sent_tokenize(i[0]):\n",
    "            words = [word.lower() for word in TreebankWordTokenizer().tokenize(sentence) if word.lower() not in stop_word_tuple and re.search(\"^[a-zA-Z]+$\", word)]\n",
    "            #words = ' '.join([SnowballStemmer(\"english\").stem(word).lower() for word in words])\n",
    "            wordstems = [SnowballStemmer(\"english\").stem(word).lower() for word in words]\n",
    "            doc.append(' '.join(wordstems))\n",
    "            for word in wordstems:\n",
    "                all_words.append(word)\n",
    "        #print '-'\n",
    "        train_set_proc.append(doc)\n",
    "    #tuples dont support assignment so we have to re-create trainset with processed\n",
    "    string_class_tuple = [ (train_set_proc[idx],i[1]) for idx,i in enumerate(string_class_tuple)]\n",
    "    return all_words, string_class_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list of common stop words\n",
    "stop_word_tuple = tuple(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### create our bag of words and pre-processed data set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words, proc_train_set = pre_proc(pres_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implement our classify: defining features we will use to determine which class a tweet is from. We have several features that have been chosen for various reasons which can be seen here (link to readme)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define a feature extractor \n",
    "def document_features(document,word_features):\n",
    "    document_words = set(' '.join(document).split())\n",
    "    #document_words = set(document))\n",
    "    #print document_words\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "#Use significant bigrams i.e. measure the chi-square collocation correlation \n",
    "#between words in the bigram. Here we use the top N bigrams as features for \n",
    "#our classifier.\n",
    "def document_bigram_features(document, N):\n",
    "    #Use significant bigrams i.e. measure the chi-square collocation correlation \n",
    "    #between words in the bigram. Here we use the top N bigrams as features for \n",
    "    #our classifier\n",
    "    #followed this example http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "    def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=N):\n",
    "        bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "        bigrams = bigram_finder.nbest(score_fn, n)\n",
    "        return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "    document_words = set(' '.join(document).split())\n",
    "    return bigram_word_feats(document_words)\n",
    "\n",
    "#Try a different strategy seperating positive and negative labled docs wordlist\n",
    "#using a scoring method for feature selection based on a chi-square test that \n",
    "#eliminates low-information word features by using information gain for each word\n",
    "#followed this example http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/\n",
    "def eliminate_low_noninfo_fs(train, N):\n",
    "    #creates lists of all positive and negative words\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    for doc in train:\n",
    "        if doc[1] == 'trump':\n",
    "            for sent in doc[0]:\n",
    "                pos_list.append(sent.split())\n",
    "        elif doc[1] == 'obama':\n",
    "            for sent in doc[0]:\n",
    "                neg_list.append(sent.split())\n",
    "    #create itertool chain objects with the word lists\t\n",
    "    pos_list = list(itertools.chain(*pos_list))\n",
    "    neg_list = list(itertools.chain(*neg_list))\n",
    "    #build frequency distibutions for pos, neg, and all words\n",
    "    word_fd = nltk.probability.FreqDist()\n",
    "    cond_word_fd = nltk.probability.ConditionalFreqDist()\n",
    "    for word in pos_list:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['trump'][word] += 1\n",
    "    for word in neg_list:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['obama'][word] += 1\n",
    "    #counts for pos, neg, and all words\n",
    "    pos_word_count = cond_word_fd['trump'].N()\n",
    "    neg_word_count = cond_word_fd['obama'].N()\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "    #populate score dictionary with pos, neg, and all words chi-square scores\n",
    "    word_scores = {}\n",
    "    for word, freq in word_fd.iteritems():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['trump'][word], (freq, pos_word_count), total_word_count)\n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['obama'][word], (freq, neg_word_count), total_word_count)\n",
    "        word_scores[word] = pos_score + neg_score\n",
    "    #pick the top most informative words based on the calculated scores\n",
    "    #NOTE: vary this parameter to see how our classifier performs\n",
    "    #EDIT: made the function take N as an input\n",
    "    best_vals = sorted(word_scores.iteritems(), key=lambda (w, s): s, reverse=True)[:N]\n",
    "    best_words = set([w for w, s in best_vals])\n",
    "    return best_words\n",
    "    \n",
    "def evaluate_clssifier(classifier,eval_set):\n",
    "    print('classifier accuracy: ' + str(nltk.classify.accuracy(classifier, eval_set)))\n",
    "    classifier.show_most_informative_features(20)\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    for i, (feats, label) in enumerate(eval_set):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    print 'pos precision:', precision(refsets['trump'], testsets['trump']) \n",
    "    print 'pos recall:', recall(refsets['trump'], testsets['trump'])\n",
    "    print 'pos F-measure:', f_measure(refsets['trump'], testsets['trump'])\n",
    "    print 'neg precision:', precision(refsets['obama'], testsets['obama'])\n",
    "    print 'neg recall:', recall(refsets['obama'], testsets['obama'])\n",
    "    print 'neg F-measure:', f_measure(refsets['obama'], testsets['obama'])\n",
    "\n",
    "#Here we define our function to run and test the Naive Bayes classifier\n",
    "#with different feature sets\n",
    "def word_feature_model(train, all_words, N, BI=False):\n",
    "    if BI==True:\n",
    "        #split the data into train and dev\n",
    "        train, dev = train[:367], train[368:459]\n",
    "        train = [(document_bigram_features(d,N), c) for (d,c) in train]\n",
    "        dev = [(document_bigram_features(d,N), c) for (d,c) in dev]\n",
    "        #use nltks prepackaged Naive Bayes classifier\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "        #Finall, we evaluate the classifier's performance metrics!\n",
    "        evaluate_clssifier(classifier,dev)\n",
    "    else:\n",
    "        #define our first feature: calculate N most frequent words in our corpus,\n",
    "        #then check if N words are present in each review\n",
    "        #followed the example in nltk book ch.6\n",
    "        all_words = nltk.FreqDist(w for w in all_words)\n",
    "        #create our feature list\n",
    "        #NOTE: we are using whatver word features are input for all_words parameter\n",
    "        word_features = list(all_words) \n",
    "        \n",
    "        #split the data into train and dev\n",
    "        train, dev = train[:367], train[368:459]\n",
    "        \n",
    "        #invoke the feature extractor on each processed movie reviews\n",
    "        train = [(document_features(d,word_features), c) for (d,c) in train]\n",
    "        dev = [(document_features(d,word_features), c) for (d,c) in dev]\n",
    "\n",
    "        #use nltks prepackaged Naive Bayes classifier\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "        #Finall, we evaluate the classifier's performance metrics!\n",
    "        evaluate_clssifier(classifier,dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at what words (stems/tokens) our feature extractor determines as the top 2000 most informatiove words in the `pres_corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'abus',\n",
       " u'accept',\n",
       " u'accus',\n",
       " u'accusrd',\n",
       " u'action',\n",
       " u'actual',\n",
       " u'administr',\n",
       " u'admit',\n",
       " u'advis',\n",
       " u'afraid',\n",
       " u'ag',\n",
       " u'agenc',\n",
       " u'ago',\n",
       " u'aircraft',\n",
       " u'alien',\n",
       " u'aliv',\n",
       " u'alreadi',\n",
       " u'also',\n",
       " u'alway',\n",
       " u'amazon',\n",
       " u'amen',\n",
       " u'america',\n",
       " u'american',\n",
       " u'americano',\n",
       " u'amount',\n",
       " u'amp',\n",
       " u'angrili',\n",
       " u'anonym',\n",
       " u'anoth',\n",
       " u'answer',\n",
       " u'anyon',\n",
       " u'appoint',\n",
       " u'approv',\n",
       " u'asham',\n",
       " u'ask',\n",
       " u'ass',\n",
       " u'asshol',\n",
       " u'attack',\n",
       " u'attent',\n",
       " u'attorney',\n",
       " u'away',\n",
       " u'awhil',\n",
       " u'back',\n",
       " u'bad',\n",
       " u'ball',\n",
       " u'bankrupt',\n",
       " u'barrier',\n",
       " u'bbcworld',\n",
       " u'becom',\n",
       " u'bed',\n",
       " u'behind',\n",
       " u'believ',\n",
       " u'bet',\n",
       " u'big',\n",
       " u'biggest',\n",
       " u'bitch',\n",
       " u'bitter',\n",
       " u'black',\n",
       " u'bless',\n",
       " u'blind',\n",
       " u'book',\n",
       " u'border',\n",
       " u'boy',\n",
       " u'brain',\n",
       " u'break',\n",
       " u'bring',\n",
       " u'broken',\n",
       " u'build',\n",
       " u'bulli',\n",
       " u'bunni',\n",
       " u'bus',\n",
       " u'busi',\n",
       " u'buy',\n",
       " u'cabal',\n",
       " u'cabinet',\n",
       " u'call',\n",
       " u'came',\n",
       " u'canada',\n",
       " u'caravan',\n",
       " u'carri',\n",
       " u'chanc',\n",
       " u'chang',\n",
       " u'chief',\n",
       " u'china',\n",
       " u'chronicpain',\n",
       " u'civilian',\n",
       " u'classifi',\n",
       " u'clear',\n",
       " u'client',\n",
       " u'clinton',\n",
       " u'closet',\n",
       " u'cohen',\n",
       " u'cohenmobsterlawy',\n",
       " u'coincid',\n",
       " u'come',\n",
       " u'comey',\n",
       " u'committe',\n",
       " u'competit',\n",
       " u'comradetrump',\n",
       " u'congenit',\n",
       " u'congress',\n",
       " u'conserv',\n",
       " u'control',\n",
       " u'corrupt',\n",
       " u'cost',\n",
       " u'could',\n",
       " u'count',\n",
       " u'countri',\n",
       " u'coutri',\n",
       " u'cpp',\n",
       " u'cradl',\n",
       " u'credibl',\n",
       " u'crime',\n",
       " u'crimin',\n",
       " u'crook',\n",
       " u'currenc',\n",
       " u'curtain',\n",
       " u'daili',\n",
       " u'damn',\n",
       " u'day',\n",
       " u'degre',\n",
       " u'deliri',\n",
       " u'deliveri',\n",
       " u'dem',\n",
       " u'democrat',\n",
       " u'demon',\n",
       " u'depart',\n",
       " u'deriv',\n",
       " u'describ',\n",
       " u'deserv',\n",
       " u'design',\n",
       " u'despis',\n",
       " u'destroy',\n",
       " u'devalu',\n",
       " u'die',\n",
       " u'digenova',\n",
       " u'diplomat',\n",
       " u'director',\n",
       " u'disast',\n",
       " u'disgrac',\n",
       " u'disgruntl',\n",
       " u'dishonest',\n",
       " u'document',\n",
       " u'doj',\n",
       " u'dollar',\n",
       " u'donald',\n",
       " u'donaldjtrumpjr',\n",
       " u'donni',\n",
       " u'dont',\n",
       " u'donthecon',\n",
       " u'dose',\n",
       " u'douch',\n",
       " u'draft',\n",
       " u'drain',\n",
       " u'drivel',\n",
       " u'drop',\n",
       " u'dumper',\n",
       " u'easili',\n",
       " u'economi',\n",
       " u'ef',\n",
       " u'effen',\n",
       " u'either',\n",
       " u'elect',\n",
       " u'els',\n",
       " u'embarrass',\n",
       " u'end',\n",
       " u'enjoy',\n",
       " u'enrag',\n",
       " u'era',\n",
       " u'eua',\n",
       " u'europ',\n",
       " u'even',\n",
       " u'ever',\n",
       " u'everi',\n",
       " u'everybodi',\n",
       " u'everyon',\n",
       " u'evid',\n",
       " u'exact',\n",
       " u'excus',\n",
       " u'execut',\n",
       " u'exoner',\n",
       " u'expect',\n",
       " u'exploit',\n",
       " u'expos',\n",
       " u'eye',\n",
       " u'f',\n",
       " u'fact',\n",
       " u'factor',\n",
       " u'fake',\n",
       " u'famili',\n",
       " u'far',\n",
       " u'fast',\n",
       " u'fault',\n",
       " u'fbi',\n",
       " u'feel',\n",
       " u'figur',\n",
       " u'find',\n",
       " u'fiquei',\n",
       " u'fire',\n",
       " u'first',\n",
       " u'fixer',\n",
       " u'flotus',\n",
       " u'flush',\n",
       " u'follow',\n",
       " u'fool',\n",
       " u'foolish',\n",
       " u'forward',\n",
       " u'founder',\n",
       " u'four',\n",
       " u'foxnew',\n",
       " u'franc',\n",
       " u'fraud',\n",
       " u'friend',\n",
       " u'fuck',\n",
       " u'g',\n",
       " u'game',\n",
       " u'garbag',\n",
       " u'gave',\n",
       " u'general',\n",
       " u'get',\n",
       " u'giant',\n",
       " u'give',\n",
       " u'glad',\n",
       " u'go',\n",
       " u'god',\n",
       " u'goebbel',\n",
       " u'good',\n",
       " u'got',\n",
       " u'govern',\n",
       " u'great',\n",
       " u'guy',\n",
       " u'hahahaha',\n",
       " u'handl',\n",
       " u'hanniti',\n",
       " u'happen',\n",
       " u'hard',\n",
       " u'head',\n",
       " u'heard',\n",
       " u'help',\n",
       " u'hide',\n",
       " u'higher',\n",
       " u'hilari',\n",
       " u'hillari',\n",
       " u'hit',\n",
       " u'hitler',\n",
       " u'honest',\n",
       " u'horrend',\n",
       " u'hous',\n",
       " u'housedemocrat',\n",
       " u'housegop',\n",
       " u'howardsternjohn',\n",
       " u'https',\n",
       " u'huge',\n",
       " u'hurri',\n",
       " u'hurt',\n",
       " u'hypocrisi',\n",
       " u'idea',\n",
       " u'idiot',\n",
       " u'ignor',\n",
       " u'iii',\n",
       " u'illeg',\n",
       " u'immigr',\n",
       " u'impeach',\n",
       " u'includ',\n",
       " u'incompet',\n",
       " u'inform',\n",
       " u'innat',\n",
       " u'innoc',\n",
       " u'intemper',\n",
       " u'interest',\n",
       " u'interrupt',\n",
       " u'ion',\n",
       " u'iran',\n",
       " u'israel',\n",
       " u'issu',\n",
       " u'ivankatrump',\n",
       " u'jackass',\n",
       " u'jail',\n",
       " u'jame',\n",
       " u'jeff',\n",
       " u'jeffsess',\n",
       " u'joe',\n",
       " u'joker',\n",
       " u'judg',\n",
       " u'judiciari',\n",
       " u'justic',\n",
       " u'keep',\n",
       " u'kick',\n",
       " u'kid',\n",
       " u'kill',\n",
       " u'kind',\n",
       " u'kingdom',\n",
       " u'knew',\n",
       " u'know',\n",
       " u'known',\n",
       " u'labor',\n",
       " u'larg',\n",
       " u'lash',\n",
       " u'last',\n",
       " u'latest',\n",
       " u'laugh',\n",
       " u'launch',\n",
       " u'law',\n",
       " u'lawmak',\n",
       " u'lawyer',\n",
       " u'lead',\n",
       " u'leader',\n",
       " u'leadership',\n",
       " u'leak',\n",
       " u'left',\n",
       " u'let',\n",
       " u'liar',\n",
       " u'liber',\n",
       " u'lie',\n",
       " u'like',\n",
       " u'linden',\n",
       " u'line',\n",
       " u'lip',\n",
       " u'lire',\n",
       " u'littl',\n",
       " u'live',\n",
       " u'lock',\n",
       " u'lockhimup',\n",
       " u'long',\n",
       " u'look',\n",
       " u'loos',\n",
       " u'loser',\n",
       " u'lost',\n",
       " u'lot',\n",
       " u'love',\n",
       " u'lover',\n",
       " u'loyalti',\n",
       " u'lynch',\n",
       " u'lyric',\n",
       " u'major',\n",
       " u'make',\n",
       " u'man',\n",
       " u'mandar',\n",
       " u'mani',\n",
       " u'marin',\n",
       " u'mas',\n",
       " u'massiv',\n",
       " u'mate',\n",
       " u'matter',\n",
       " u'mccabe',\n",
       " u'mean',\n",
       " u'media',\n",
       " u'mere',\n",
       " u'mess',\n",
       " u'mexico',\n",
       " u'michael',\n",
       " u'miley',\n",
       " u'million',\n",
       " u'miss',\n",
       " u'missil',\n",
       " u'mission',\n",
       " u'mobster',\n",
       " u'mom',\n",
       " u'money',\n",
       " u'moral',\n",
       " u'moron',\n",
       " u'mother',\n",
       " u'motherfuck',\n",
       " u'mount',\n",
       " u'mr',\n",
       " u'much',\n",
       " u'murder',\n",
       " u'must',\n",
       " u'name',\n",
       " u'nation',\n",
       " u'need',\n",
       " u'negat',\n",
       " u'never',\n",
       " u'news',\n",
       " u'night',\n",
       " u'noth',\n",
       " u'notic',\n",
       " u'oath',\n",
       " u'obama',\n",
       " u'obamag',\n",
       " u'obvious',\n",
       " u'occur',\n",
       " u'offic',\n",
       " u'oh',\n",
       " u'ok',\n",
       " u'old',\n",
       " u'one',\n",
       " u'oop',\n",
       " u'opioidcrisi',\n",
       " u'opioidepidem',\n",
       " u'opioidhysteria',\n",
       " u'os',\n",
       " u'other',\n",
       " u'otherwis',\n",
       " u'outro',\n",
       " u'pack',\n",
       " u'pal',\n",
       " u'pant',\n",
       " u'paranoia',\n",
       " u'past',\n",
       " u'pay',\n",
       " u'peopk',\n",
       " u'peopl',\n",
       " u'perfect',\n",
       " u'person',\n",
       " u'phoni',\n",
       " u'pictur',\n",
       " u'play',\n",
       " u'playboy',\n",
       " u'pleas',\n",
       " u'point',\n",
       " u'poll',\n",
       " u'pop',\n",
       " u'post',\n",
       " u'potus',\n",
       " u'power',\n",
       " u'prayer',\n",
       " u'precis',\n",
       " u'prepar',\n",
       " u'presid',\n",
       " u'president',\n",
       " u'presidenti',\n",
       " u'pressur',\n",
       " u'prevail',\n",
       " u'privileg',\n",
       " u'probabl',\n",
       " u'proof',\n",
       " u'protect',\n",
       " u'proven',\n",
       " u'public',\n",
       " u'punish',\n",
       " u'putin',\n",
       " u'quack',\n",
       " u'que',\n",
       " u'question',\n",
       " u'quit',\n",
       " u'quiz',\n",
       " u'raid',\n",
       " u'rais',\n",
       " u'rake',\n",
       " u'rapist',\n",
       " u'rasmussen',\n",
       " u'rate',\n",
       " u'real',\n",
       " u'realdonaldtrump',\n",
       " u'realiti',\n",
       " u'realli',\n",
       " u'reduc',\n",
       " u'rememb',\n",
       " u'remind',\n",
       " u'report',\n",
       " u'repub',\n",
       " u'request',\n",
       " u'rest',\n",
       " u'return',\n",
       " u'review',\n",
       " u'rid',\n",
       " u'right',\n",
       " u'rise',\n",
       " u'risk',\n",
       " u'round',\n",
       " u'rt',\n",
       " u'rule',\n",
       " u'russia',\n",
       " u'russiansanct',\n",
       " u'russiansanctionsnow',\n",
       " u'sabendo',\n",
       " u'sad',\n",
       " u'safeti',\n",
       " u'sanction',\n",
       " u'sanctuari',\n",
       " u'satan',\n",
       " u'say',\n",
       " u'scare',\n",
       " u'screw',\n",
       " u'scumbag',\n",
       " u'scummiest',\n",
       " u'seal',\n",
       " u'sean',\n",
       " u'seanhann',\n",
       " u'second',\n",
       " u'section',\n",
       " u'secur',\n",
       " u'see',\n",
       " u'sei',\n",
       " u'senat',\n",
       " u'senatedem',\n",
       " u'senategop',\n",
       " u'senhor',\n",
       " u'sent',\n",
       " u'serv',\n",
       " u'session',\n",
       " u'sheep',\n",
       " u'shes',\n",
       " u'ship',\n",
       " u'show',\n",
       " u'shut',\n",
       " u'shyt',\n",
       " u'sieg',\n",
       " u'sinc',\n",
       " u'sink',\n",
       " u'sir',\n",
       " u'skeleton',\n",
       " u'slaw',\n",
       " u'slick',\n",
       " u'slime',\n",
       " u'slipperi',\n",
       " u'small',\n",
       " u'smart',\n",
       " u'soab',\n",
       " u'someon',\n",
       " u'someth',\n",
       " u'son',\n",
       " u'song',\n",
       " u'sourc',\n",
       " u'southern',\n",
       " u'spineless',\n",
       " u'spoken',\n",
       " u'spokesperson',\n",
       " u'spout',\n",
       " u'star',\n",
       " u'start',\n",
       " u'state',\n",
       " u'still',\n",
       " u'stock',\n",
       " u'stood',\n",
       " u'stop',\n",
       " u'stori',\n",
       " u'stormi',\n",
       " u'stormydaniel',\n",
       " u'stormyweath',\n",
       " u'strang',\n",
       " u'strike',\n",
       " u'strong',\n",
       " u'stupid',\n",
       " u'support',\n",
       " u'sure',\n",
       " u'surplus',\n",
       " u'suspicion',\n",
       " u'syria',\n",
       " u'syrian',\n",
       " u'take',\n",
       " u'talk',\n",
       " u'tantrum',\n",
       " u'tariff',\n",
       " u'tarmac',\n",
       " u'tax',\n",
       " u'tell',\n",
       " u'tema',\n",
       " u'temper',\n",
       " u'term',\n",
       " u'terribl',\n",
       " u'thank',\n",
       " u'thing',\n",
       " u'think',\n",
       " u'thought',\n",
       " u'three',\n",
       " u'throw',\n",
       " u'thumper',\n",
       " u'thursday',\n",
       " u'time',\n",
       " u'tini',\n",
       " u'tire',\n",
       " u'toddler',\n",
       " u'togeth',\n",
       " u'tonight',\n",
       " u'total',\n",
       " u'trade',\n",
       " u'traitor',\n",
       " u'trash',\n",
       " u'treason',\n",
       " u'tremend',\n",
       " u'truer',\n",
       " u'trump',\n",
       " u'trumper',\n",
       " u'trumpi',\n",
       " u'trumprussia',\n",
       " u'trumpspervert',\n",
       " u'trumptreason',\n",
       " u'truth',\n",
       " u'turd',\n",
       " u'turn',\n",
       " u'tv',\n",
       " u'tweet',\n",
       " u'twitter',\n",
       " u'two',\n",
       " u'u',\n",
       " u'ugli',\n",
       " u'uh',\n",
       " u'un',\n",
       " u'unbeliev',\n",
       " u'uneth',\n",
       " u'unfair',\n",
       " u'unfit',\n",
       " u'unit',\n",
       " u'unknow',\n",
       " u'unlik',\n",
       " u'upset',\n",
       " u'us',\n",
       " u'usa',\n",
       " u'use',\n",
       " u'ut',\n",
       " u'valu',\n",
       " u'vent',\n",
       " u'vile',\n",
       " u'vote',\n",
       " u'vp',\n",
       " u'w',\n",
       " u'wait',\n",
       " u'wall',\n",
       " u'want',\n",
       " u'war',\n",
       " u'watch',\n",
       " u'way',\n",
       " u'weak',\n",
       " u'well',\n",
       " u'whack',\n",
       " u'whe',\n",
       " u'whoopsi',\n",
       " u'wi',\n",
       " u'willi',\n",
       " u'willing',\n",
       " u'wing',\n",
       " u'wisdom',\n",
       " u'withdraw',\n",
       " u'without',\n",
       " u'wo',\n",
       " u'wonder',\n",
       " u'word',\n",
       " u'work',\n",
       " u'worker',\n",
       " u'world',\n",
       " u'worst',\n",
       " u'worthless',\n",
       " u'would',\n",
       " u'wow',\n",
       " u'wrong',\n",
       " u'year',\n",
       " u'yike',\n",
       " u'yo'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eliminate_low_noninfo_fs(proc_train_set,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### use the word_feature_model wrapper to run our classification task on our pre-processed data using various methods including using the 2000 most informative words as predictive features!\n",
    "> what kind of features are used by the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try some of our methods out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier accuracy: 1.0\n",
      "Most Informative Features\n",
      "           contains(get) = True            obama : trump  =    119.7 : 1.0\n",
      "           contains(use) = True            obama : trump  =     71.8 : 1.0\n",
      "           contains(see) = True            obama : trump  =     71.8 : 1.0\n",
      "       contains(corrupt) = True            obama : trump  =     71.8 : 1.0\n",
      "         contains(https) = True            obama : trump  =      2.8 : 1.0\n",
      "            contains(rt) = False           obama : trump  =      1.7 : 1.0\n",
      "         contains(comey) = False           obama : trump  =      1.2 : 1.0\n",
      "         contains(crook) = False           obama : trump  =      1.2 : 1.0\n",
      "           contains(lie) = False           obama : trump  =      1.2 : 1.0\n",
      "       contains(hillari) = False           obama : trump  =      1.1 : 1.0\n",
      "      contains(congress) = False           obama : trump  =      1.1 : 1.0\n",
      "          contains(long) = False           obama : trump  =      1.1 : 1.0\n",
      "             contains(g) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(senat) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(russia) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(exoner) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(draft) = False           obama : trump  =      1.1 : 1.0\n",
      "          contains(talk) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(china) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(devalu) = False           obama : trump  =      1.1 : 1.0\n",
      "pos precision: 1.0\n",
      "pos recall: 1.0\n",
      "pos F-measure: 1.0\n",
      "neg precision: 1.0\n",
      "neg recall: 1.0\n",
      "neg F-measure: 1.0\n",
      "classifier accuracy: 1.0\n",
      "Most Informative Features\n",
      "           contains(get) = True            obama : trump  =    119.7 : 1.0\n",
      "           contains(use) = True            obama : trump  =     71.8 : 1.0\n",
      "           contains(see) = True            obama : trump  =     71.8 : 1.0\n",
      "       contains(corrupt) = True            obama : trump  =     71.8 : 1.0\n",
      "         contains(https) = True            obama : trump  =      2.8 : 1.0\n",
      "            contains(rt) = False           obama : trump  =      1.7 : 1.0\n",
      "         contains(comey) = False           obama : trump  =      1.2 : 1.0\n",
      "         contains(crook) = False           obama : trump  =      1.2 : 1.0\n",
      "           contains(lie) = False           obama : trump  =      1.2 : 1.0\n",
      "       contains(hillari) = False           obama : trump  =      1.1 : 1.0\n",
      "      contains(congress) = False           obama : trump  =      1.1 : 1.0\n",
      "          contains(long) = False           obama : trump  =      1.1 : 1.0\n",
      "             contains(g) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(senat) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(russia) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(exoner) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(draft) = False           obama : trump  =      1.1 : 1.0\n",
      "          contains(talk) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(china) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(devalu) = False           obama : trump  =      1.1 : 1.0\n",
      "pos precision: 1.0\n",
      "pos recall: 1.0\n",
      "pos F-measure: 1.0\n",
      "neg precision: 1.0\n",
      "neg recall: 1.0\n",
      "neg F-measure: 1.0\n",
      "classifier accuracy: 1.0\n",
      "Most Informative Features\n",
      "           contains(get) = True            obama : trump  =    119.7 : 1.0\n",
      "           contains(use) = True            obama : trump  =     71.8 : 1.0\n",
      "           contains(see) = True            obama : trump  =     71.8 : 1.0\n",
      "       contains(corrupt) = True            obama : trump  =     71.8 : 1.0\n",
      "         contains(https) = True            obama : trump  =      2.8 : 1.0\n",
      "            contains(rt) = False           obama : trump  =      1.7 : 1.0\n",
      "         contains(comey) = False           obama : trump  =      1.2 : 1.0\n",
      "         contains(crook) = False           obama : trump  =      1.2 : 1.0\n",
      "           contains(lie) = False           obama : trump  =      1.2 : 1.0\n",
      "       contains(hillari) = False           obama : trump  =      1.1 : 1.0\n",
      "      contains(congress) = False           obama : trump  =      1.1 : 1.0\n",
      "          contains(long) = False           obama : trump  =      1.1 : 1.0\n",
      "             contains(g) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(senat) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(russia) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(exoner) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(draft) = False           obama : trump  =      1.1 : 1.0\n",
      "          contains(talk) = False           obama : trump  =      1.1 : 1.0\n",
      "         contains(china) = False           obama : trump  =      1.1 : 1.0\n",
      "        contains(devalu) = False           obama : trump  =      1.1 : 1.0\n",
      "pos precision: 1.0\n",
      "pos recall: 1.0\n",
      "pos F-measure: 1.0\n",
      "neg precision: 1.0\n",
      "neg recall: 1.0\n",
      "neg F-measure: 1.0\n",
      "classifier accuracy: 0.945054945055\n",
      "Most Informative Features\n",
      "                     get = True            obama : trump  =    119.7 : 1.0\n",
      "                     use = True            obama : trump  =     71.8 : 1.0\n",
      "                 corrupt = True            obama : trump  =     71.8 : 1.0\n",
      "                     see = True            obama : trump  =     71.8 : 1.0\n",
      "                   https = True            obama : trump  =      2.8 : 1.0\n",
      "                      rt = None            obama : trump  =      1.7 : 1.0\n",
      "                   comey = None            obama : trump  =      1.2 : 1.0\n",
      "                   crook = None            obama : trump  =      1.2 : 1.0\n",
      "                     lie = None            obama : trump  =      1.2 : 1.0\n",
      "(u'https', u'realdonaldtrump') = None            obama : trump  =      1.2 : 1.0\n",
      "                 hillari = None            obama : trump  =      1.1 : 1.0\n",
      "                congress = None            obama : trump  =      1.1 : 1.0\n",
      "(u'realdonaldtrump', u'crook') = None            obama : trump  =      1.1 : 1.0\n",
      "                       g = None            obama : trump  =      1.1 : 1.0\n",
      "                   senat = None            obama : trump  =      1.1 : 1.0\n",
      "         (u'rt', u'lie') = None            obama : trump  =      1.1 : 1.0\n",
      "                    long = None            obama : trump  =      1.1 : 1.0\n",
      "     (u'g', u'congress') = None            obama : trump  =      1.1 : 1.0\n",
      "(u'congress', u'exoner') = None            obama : trump  =      1.1 : 1.0\n",
      "(u'long', u'realdonaldtrump') = None            obama : trump  =      1.1 : 1.0\n",
      "pos precision: 1.0\n",
      "pos recall: 0.897959183673\n",
      "pos F-measure: 0.94623655914\n",
      "neg precision: 0.893617021277\n",
      "neg recall: 1.0\n",
      "neg F-measure: 0.943820224719\n"
     ]
    }
   ],
   "source": [
    "#1: Naive method: use all the words as features to classify movie reviews\n",
    "word_feature_model(proc_train_set,all_words,None)\n",
    "\n",
    "#2: Method using the top N frequent occuring words to classify movie revs\n",
    "word_feature_model(proc_train_set,all_words[:2000],None)\n",
    "\n",
    "#3: Method using the top N 'most informative' words to classify each movie rev\n",
    "word_feature_model(proc_train_set,eliminate_low_noninfo_fs(proc_train_set,2000),None)\n",
    "\n",
    "#4: Method using the bigram collocations of the top N bigrams\n",
    "word_feature_model(proc_train_set,None,200,True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
